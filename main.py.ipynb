{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Install necessary libraries\n",
        "!pip install openai pinecone-client sentence-transformers pandas"
      ],
      "metadata": {
        "collapsed": true,
        "id": "yF6YjaS5Gyxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import openai\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "\n",
        "# Initialize OpenAI API key\n",
        "openai.api_key = \"pcsk_2z3Eyk_SbfQ6dQdBwCq1oJgoenG3i8CmzqirYEhjwQNKQy1U8AwpissfLtThKhackyAd7P\"\n",
        "\n",
        "# Initialize Pinecone\n",
        "pc = Pinecone(api_key=\"pcsk_4EU7iH_GS7g1yetLN6YG2NcQeMBeGMLfnQRPGmUUAqVUdgM79Km9ZveCmeDf3YcFvsBpbG\")\n",
        "\n",
        "# Specify the environment (e.g., AWS)\n",
        "spec = ServerlessSpec(\n",
        "    cloud=\"aws\",\n",
        "    region=\"us-west-2\"  # Adjust region as needed\n",
        ")\n",
        "\n",
        "# Check if the index exists; create it if it doesn't\n",
        "index_name = \"business-qa\"\n",
        "if index_name not in pc.list_indexes().names():\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1536,  # Embedding dimension size\n",
        "        metric=\"cosine\",\n",
        "        spec=spec\n",
        "    )\n",
        "\n",
        "# Connect to the index\n",
        "index = pc.index(index_name)\n",
        "\n",
        "# Upload the dataset\n",
        "uploaded = files.upload()  # Upload a file named 'data.csv'\n",
        "\n",
        "# Load and preprocess data\n",
        "def preprocess_data(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    chunks = []\n",
        "    for idx, text in enumerate(data['content']):\n",
        "        words = text.split()\n",
        "        for i in range(0, len(words), 300):  # Split text into chunks of 300 words\n",
        "            chunk = \" \".join(words[i:i + 300])\n",
        "            chunks.append((f\"{idx}-{i//300}\", chunk))\n",
        "    return chunks\n",
        "\n",
        "file_path = next(iter(uploaded))  # Get uploaded file name\n",
        "chunks = preprocess_data(file_path)\n",
        "print(f\"Total Chunks Processed: {len(chunks)}\")\n",
        "\n",
        "# Load the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Generate embeddings for chunks\n",
        "embeddings = model.encode([chunk[1] for chunk in chunks], show_progress_bar=True)\n",
        "\n",
        "# Prepare and upload data to Pinecone\n",
        "to_upsert = [(chunk[0], emb, {\"text\": chunk[1]}) for chunk, emb in zip(chunks, embeddings)]\n",
        "index.upsert(to_upsert)\n",
        "print(f\"Uploaded {len(to_upsert)} embeddings to Pinecone.\")\n",
        "\n",
        "# Define the Retrieval Augmented Generation (RAG) pipeline\n",
        "def rag_pipeline(query, top_k=5):\n",
        "    # Generate embedding for the query\n",
        "    query_emb = model.encode([query])[0]\n",
        "\n",
        "    # Retrieve relevant documents from Pinecone\n",
        "    results = index.query(query_emb, top_k=top_k, include_metadata=True)\n",
        "    context = \" \".join([match['metadata']['text'] for match in results['matches']])\n",
        "\n",
        "    # Generate response using OpenAI\n",
        "    prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
        "    response = openai.Completion.create(\n",
        "        model=\"gpt-4\",\n",
        "        prompt=prompt,\n",
        "        temperature=0.7,\n",
        "        max_tokens=300\n",
        "    )\n",
        "    return response['choices'][0]['text'].strip()\n",
        "\n",
        "# Test the RAG QA Bot\n",
        "query = \"How do I update my account details?\"\n",
        "answer = rag_pipeline(query)\n",
        "print(f\"Q: {query}\\nA: {answer}\")\n"
      ],
      "metadata": {
        "id": "DmvwKjb5GVNI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k9RK803EJQ6Z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}